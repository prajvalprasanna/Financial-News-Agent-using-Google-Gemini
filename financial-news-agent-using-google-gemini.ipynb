{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Financial News Agent - Gemini (gemini-2.0-flash-lite-001)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:40:38.096096Z","iopub.execute_input":"2025-04-18T00:40:38.096378Z","iopub.status.idle":"2025-04-18T00:40:40.337033Z","shell.execute_reply.started":"2025-04-18T00:40:38.096351Z","shell.execute_reply":"2025-04-18T00:40:40.336082Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install feedparser\n!pip install -q google-generativeai PyMuPDF sentence-transformers faiss-cpu gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:40:40.338997Z","iopub.execute_input":"2025-04-18T00:40:40.339428Z","iopub.status.idle":"2025-04-18T00:42:36.469994Z","shell.execute_reply.started":"2025-04-18T00:40:40.339403Z","shell.execute_reply":"2025-04-18T00:42:36.468205Z"}},"outputs":[{"name":"stdout","text":"Collecting feedparser\n  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\nCollecting sgmllib3k (from feedparser)\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=36994058193a4552d2f2bb0afe5d150ebe01300e2a94374449069565e3b914fc\n  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\nSuccessfully built sgmllib3k\nInstalling collected packages: sgmllib3k, feedparser\nSuccessfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m56.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Import the required libraries","metadata":{}},{"cell_type":"code","source":"# Libraries for Setup & Configuration\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n# Libraries for Feed Parsing and Typing\nimport feedparser\nimport urllib.parse\nfrom typing import List\n# Libraries for load, save and update memory \nimport json\nfrom pathlib import Path\n# Libraries for summarizing and plotting the confidence trend\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Libraries for UI \nimport gradio as gr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:42:36.471870Z","iopub.execute_input":"2025-04-18T00:42:36.472380Z","iopub.status.idle":"2025-04-18T00:42:43.630863Z","shell.execute_reply.started":"2025-04-18T00:42:36.472330Z","shell.execute_reply":"2025-04-18T00:42:43.630028Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Configure Gemini with secure API key (from Kaggle Secrets)","metadata":{}},{"cell_type":"code","source":"genai.configure(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")) # Key value is configured in Add-ons/Secrets","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:42:43.631779Z","iopub.execute_input":"2025-04-18T00:42:43.632366Z","iopub.status.idle":"2025-04-18T00:42:43.779123Z","shell.execute_reply.started":"2025-04-18T00:42:43.632341Z","shell.execute_reply":"2025-04-18T00:42:43.778319Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Load a lightweight Gemini model suitable for quick responses","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel(\"gemini-2.0-flash-lite-001\") # Model was selected based on the rate limits of the Gemini API","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:42:43.780124Z","iopub.execute_input":"2025-04-18T00:42:43.780388Z","iopub.status.idle":"2025-04-18T00:42:43.784984Z","shell.execute_reply.started":"2025-04-18T00:42:43.780366Z","shell.execute_reply":"2025-04-18T00:42:43.784033Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Agent Memory: In-memory conversation history (used for context & memory simulation)\nconversation_history = []\n\n# Agent Functions\n\ndef rewrite_to_query(user_prompt: str) -> str:\n    prompt = f\"Extract short search keywords from: \\\"{user_prompt}\\\"\"\n    return model.generate_content(prompt).text.strip().replace(\" \", \"+\")\n\n\ndef fetch_articles(user_prompt, max_results=5):\n    #Clean the query (remove newlines, extra spaces)\n    clean_prompt = \" \".join(user_prompt.strip().split())\n    # URL encode\n    encoded_query = urllib.parse.quote_plus(clean_prompt)\n    # Build Google News RSS URL\n    rss_url = f\"https://news.google.com/rss/search?q={encoded_query}&hl=en-IN&gl=IN&ceid=IN:en\"\n    # Parse feed\n    feed = feedparser.parse(rss_url)\n    # Return top N articles\n    return feed.entries[:max_results]\n\n\ndef summarize_with_context(articles: List[dict], prompt: str, memory: str = \"\") -> List[dict]:\n    summarized = []\n    for article in articles:\n        text = f\"\"\"\nUser prompt: {prompt}\nContext: {memory}\n\nNews:\nTitle: {article['title']}\nSummary: {article['summary']}\n\nGive 3 bullet-point insights relevant to user query. Cite link: {article['link']}\n\"\"\"\n        summary = model.generate_content(text).text\n        summarized.append({\"title\": article['title'], \"summary\": summary, \"link\": article['link']})\n    return summarized\n\n\ndef generate_plan(summaries: List[dict], prompt: str, memory: str) -> str:\n    text = f\"\"\"\nUser Query: {prompt}\nContext: {memory}\nSummarized News:\n{''.join([s['summary'] for s in summaries])}\n\nGive output as function: investment_plan(decision, reasons, confidence_score)\n\"\"\"\n    return model.generate_content(text).text.strip()\n\n\ndef evaluate_plan(plan: str) -> str:\n    text = f\"\"\"\nEvaluate this investment_plan:\n{plan}\n\nRate 1-10 with reasoning.\n\"\"\"\n    return model.generate_content(text).text.strip()\n\n\n# Master Agent Function (Loop-Aware)\n\ndef financial_agent(user_prompt: str, rag_context: str = \"\"):\n    # Add to memory\n    conversation_history.append({\"role\": \"user\", \"content\": user_prompt})\n    context_memory = \" \".join([item[\"content\"] for item in conversation_history if item[\"role\"] == \"user\"])\n\n    # Convert to search query\n    query = rewrite_to_query(user_prompt)\n\n    # Fetch news\n    articles = fetch_articles(query)\n\n    # Summarize with memory\n    summaries = summarize_with_context(articles, user_prompt, context_memory)\n\n    # Action plan\n    action_plan = generate_plan(summaries, user_prompt, rag_context)\n\n    # Evaluation\n    evaluation = evaluate_plan(action_plan)\n\n    # Add system-generated context to memory\n    conversation_history.append({\"role\": \"agent\", \"content\": action_plan})\n\n    # Return response\n    return {\n        \"query\": query,\n        \"summaries\": summaries,\n        \"action_plan\": action_plan,\n        \"evaluation\": evaluation\n    }","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:42:43.785774Z","iopub.execute_input":"2025-04-18T00:42:43.786073Z","iopub.status.idle":"2025-04-18T00:42:43.805112Z","shell.execute_reply.started":"2025-04-18T00:42:43.786033Z","shell.execute_reply":"2025-04-18T00:42:43.804293Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"MEMORY_FILE = Path(\"agent_memory.json\")\n\ndef load_memory():\n    if MEMORY_FILE.exists():\n        with open(MEMORY_FILE, \"r\") as f:\n            return json.load(f)\n    return []\n\ndef save_memory(memory):\n    with open(MEMORY_FILE, \"w\") as f:\n        json.dump(memory, f, indent=2)\n\n# On initialization\nconversation_history = load_memory()\n\n# At end of agent run\ndef update_memory(user_prompt, agent_response):\n    conversation_history.append({\"role\": \"user\", \"content\": user_prompt})\n    conversation_history.append({\"role\": \"agent\", \"content\": agent_response})\n    save_memory(conversation_history)","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:42:43.807363Z","iopub.execute_input":"2025-04-18T00:42:43.807604Z","iopub.status.idle":"2025-04-18T00:42:43.826881Z","shell.execute_reply.started":"2025-04-18T00:42:43.807586Z","shell.execute_reply":"2025-04-18T00:42:43.826006Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def summarize_to_dataframe(summaries):\n    data = []\n    for s in summaries:\n        data.append({\n            \"Title\": s[\"title\"],\n            \"Summary\": s[\"summary\"],\n            \"Link\": s[\"link\"]\n        })\n    return pd.DataFrame(data)\n\n\ndef plot_confidence_trend(memory):\n    scores = []\n    labels = []\n    for item in memory:\n        if item[\"role\"] == \"agent\":\n            content = item[\"content\"]\n            try:\n                score = int([s for s in content.split() if s.isdigit()][-1])\n                scores.append(score)\n                labels.append(len(scores))\n            except:\n                continue\n    if not scores:\n        return None\n    plt.figure(figsize=(6,3))\n    plt.plot(labels, scores, marker='o')\n    plt.title(\"Confidence Scores Over Time\")\n    plt.xlabel(\"Query Number\")\n    plt.ylabel(\"Confidence Score\")\n    plt.grid(True)\n    return plt","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:42:43.827806Z","iopub.execute_input":"2025-04-18T00:42:43.828124Z","iopub.status.idle":"2025-04-18T00:42:43.847241Z","shell.execute_reply.started":"2025-04-18T00:42:43.828094Z","shell.execute_reply":"2025-04-18T00:42:43.846313Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def run_agent_interface(user_prompt, rag_context):\n    result = financial_agent(user_prompt, rag_context)\n\n    # Save to persistent memory\n    update_memory(user_prompt, result[\"action_plan\"])\n\n    # Format output\n    df = summarize_to_dataframe(result[\"summaries\"])\n    plot = plot_confidence_trend(conversation_history)\n\n    return result[\"action_plan\"], result[\"evaluation\"], df, plot\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"## ðŸ“ˆ Financial News Agent with Gemini\")\n\n    user_prompt = gr.Textbox(label=\"Your Prompt\", placeholder=\"e.g. Give me the highlights of today's market\")\n    rag_input = gr.Textbox(label=\"Domain Context (Optional)\", placeholder=\"e.g. Top Q1 earners\")\n\n    run_button = gr.Button(\"Analyze\")\n\n    with gr.Row():\n        action_output = gr.Textbox(label=\"ðŸ“Š Investment Plan\")\n        eval_output = gr.Textbox(label=\"ðŸ§ª Evaluation\")\n\n    df_output = gr.Dataframe(label=\"Summarized News\")\n    chart_output = gr.Plot(label=\"Confidence Trend\")\n\n    run_button.click(fn=run_agent_interface, \n                     inputs=[user_prompt, rag_input],\n                     outputs=[action_output, eval_output, df_output, chart_output])","metadata":{"execution":{"iopub.status.busy":"2025-04-18T00:46:08.400762Z","iopub.execute_input":"2025-04-18T00:46:08.401240Z","iopub.status.idle":"2025-04-18T00:46:08.584693Z","shell.execute_reply.started":"2025-04-18T00:46:08.401215Z","shell.execute_reply":"2025-04-18T00:46:08.583865Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"demo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T00:46:15.744466Z","iopub.execute_input":"2025-04-18T00:46:15.744777Z","iopub.status.idle":"2025-04-18T00:46:17.055860Z","shell.execute_reply.started":"2025-04-18T00:46:15.744756Z","shell.execute_reply":"2025-04-18T00:46:17.055193Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://a715dffaca2b14377d.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://a715dffaca2b14377d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}